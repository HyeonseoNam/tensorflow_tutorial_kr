{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification with movie reivews\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 notebook은 영화리뷰의 텍스트를 이용하여 긍정인지 부정인지를 분류한다. 이러한 긍, 부정 분류는 2중 분류 (binary, or two-class) 한 예시이고, 머신러닝 문제에서 넓게 적용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사용될 데이터셋인 [IMDB dataset](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb)는:\n",
    "- 50,000 영화 리뷰 포함하며\n",
    "- training과 testing set들은 같은 각 set 내에서 같은 수의 긍정, 부정 리뷰를 담고있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서는 model을 구성하고 학습하기 하기 위한 high-level API인 `tf.keras`를 사용하며, `tf.keras`를 사용하여 text classification에 대해 더 심화된 가이드를 얻고 싶다면 [MLCC Text Classification Guide](https://developers.google.com/machine-learning/guides/text-classification/) 여기를 참고하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the IMDB dataset\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMDB dataset를 tensorflow로 다운받을 경우, 전처리가 되어있어 (단어들이 sequence 되어있음) 각 정수(각 정수는 dictionary에서 특정 단어를 표현)의 sequence로 변환되어있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 코드에서 IMDB dataset을 받아보자:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = keras.datasets.imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`num_words=1000`은 training set에서 자주 사용되는 단어 상위 10,000개를 추출하라는 argument이다. 이를 통해 데이터를 다루기 좋을 사이즈로 유지하기 위해서 드물게 사용되는 단어들은 제거됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data format을 유심히 보자. dataset이 전처리가 되어있기때문에, 각 예제는 영화 리뷰 단어들의 정수배열을 나타낸다. 각 label은 0 혹은 1인 정수를 가질 것이다. <br>*(0: negative review, 1: positive review)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training entries: 25000, labels: 25000\n"
     ]
    }
   ],
   "source": [
    "print(\"Training entries: {}, labels: {}\".format(len(train_data), len(train_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 리뷰의 텍스트는 dictionary에서 특정 단어를 표현하는 정수로 변환되어있다. 첫번째 리뷰를 보면 알 수 있다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 영화리뷰들은 각각 다른 길이들을 가질것이고 아래의 코드로 첫번째와 두번째 리뷰를 보면 확인해볼 수 있다. 그런데 뉴럴넷의 input들은 고정된 길이를 가져야 하는데 이 문제는 일단 나중에 다룰 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(218, 189)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[0]), len(train_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert the integers back to words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정수를 다시 텍스트로 변환해보자. dictionary object에 integer->string으로 매핑해주는 query를 날릴 수 있도록 helper 함수를 생성해보자:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary mapping words to an interger index\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "\"\"\"\n",
    "print(word_index)\n",
    "# { 'fawn': 34701, 'tsukino': 52006, 'nunnery': 52007, ...}\n",
    "\"\"\"\n",
    "# The first inddices are reserved\n",
    "word_index = {k:(v+3) for k,v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2 # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "# +코멘트) integer를 key로 두기 위해 key와 value를 뒤집는다.\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'second'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_word_index.get(333, '?') # 찾지 못하면 '?'로 둔다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text를 넣었을때 각 단어들을 이어붙이고 그 사이에 띄어쓰기(' ')를 넣도록 함수를 짜면:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 `decode_review` 함수로 리뷰의 텍스트를 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_review(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 리뷰(정수배열)는 뉴럴넷에 들어가기전, 반드시 tensor로 변환되어야 하고 다음의 방법들이 있다: \n",
    "- **array를 vector로변환**: one-hot encoding과 유사하게 있는 단어는 1을, 없는 단어는 0을 나타내는 vector로 변환 시킨다. 예를 들어, [3, 5]라는 한 리뷰 예제는 10,000차원(이전에 10,000의 단어만 추출하였으므로)의 vector가 되며, 이 vector에서 3과 5의 자리에만 1을, 나머지는 모두 0을 가진다. 그 후, floating point vector를 다룰 수 있는 Dense layer로 첫번째 layer를 생성하게 된다. 이런 방식은 `num_words * num_reivews` 크기의 matrix를 필요로 하는 메모리를 많이 필요로 하는 방식이다.\n",
    "- **integer tensor**: 대안적으로, 같은 형태(길이)를 갖도록 array를 덧대고, `max_length * num_reviews` 크기의 integer tensor를 생성하는 방식이다. 이런 형태를 다룰 수 있는 embedding layer를 네트워크의 첫 layer에 두면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 튜토리얼에서는 두번째 방식을 사용할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "리뷰들이 같은 길이를 가져야하므로, [pad_sequences](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences) 함수를 사용하여 길이를 규격화할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data, \n",
    "                                                       value=word_index[\"<PAD>\"],\n",
    "                                                       padding='post', \n",
    "                                                       maxlen=256)\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data, \n",
    "                                                       value=word_index[\"<PAD>\"],\n",
    "                                                       padding='post', \n",
    "                                                       maxlen=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다시 예제들을 보자:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 256)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[0]), len(train_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1   14   22   16   43  530  973 1622 1385   65  458 4468   66 3941\n",
      "    4  173   36  256    5   25  100   43  838  112   50  670    2    9\n",
      "   35  480  284    5  150    4  172  112  167    2  336  385   39    4\n",
      "  172 4536 1111   17  546   38   13  447    4  192   50   16    6  147\n",
      " 2025   19   14   22    4 1920 4613  469    4   22   71   87   12   16\n",
      "   43  530   38   76   15   13 1247    4   22   17  515   17   12   16\n",
      "  626   18    2    5   62  386   12    8  316    8  106    5    4 2223\n",
      " 5244   16  480   66 3785   33    4  130   12   16   38  619    5   25\n",
      "  124   51   36  135   48   25 1415   33    6   22   12  215   28   77\n",
      "   52    5   14  407   16   82    2    8    4  107  117 5952   15  256\n",
      "    4    2    7 3766    5  723   36   71   43  530  476   26  400  317\n",
      "   46    7    4    2 1029   13  104   88    4  381   15  297   98   32\n",
      " 2071   56   26  141    6  194 7486   18    4  226   22   21  134  476\n",
      "   26  480    5  144   30 5535   18   51   36   28  224   92   25  104\n",
      "    4  226   65   16   38 1334   88   12   16  283    5   16 4472  113\n",
      "  103   32   15   16 5345   19  178   32    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "뉴럴넷은 layer를 쌓아 형성되는데, 여기서 구조를 형성하는데 두가지의 주요한 결정사항이 있다:\n",
    "- model에 layer를 얼마나 사용할 것인가?\n",
    "- 각 layer에서 hidden unit은 얼마나 될 것인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 예제에서는, input data는 word-indeces의 배열로 구성되며, 예측할 label은 0 혹은 1이다. 이 문제에서 model을 구축해보자:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,289\n",
      "Trainable params: 160,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# input shape is the vocabulary count used for the movie reviews(10,000 words)\n",
    "vocab_size = 10000\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, 16))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(16, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "\n",
    "model.summary() # +코멘트) 모델의 구조를 보여준다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **첫번째 레이어*: `Embedding layer`, 이 layer는 정수로 인코딩된 어휘들을 받고 각 단어색인에 해당하는 embedding vector를 찾는다. 이러한 vector들이 모델이 train됨에 때라 학습된다. vector들은 출력될 array에 차원을 추가하며, 그 결과 차원들은 `batch, sequence, embedding` 이다.\n",
    "2. **두번째 레이어*: `GlobalAveragePooling1d`, 이 layer는 sequence dimension을 평균하여 각 예제에 고정적인 크기의 output vector를 리턴하는데, 이로 인해 model이 다양한 길이의 input을 다룰 수 있게 된다.\n",
    "3. **세번째 레이어*: `Dense(16, activation=tf.nn.relu)`, 이전 레이어에서 받은 고정적인 output vector는 16개의 hidden unit으로 이루어진 fully-connected layer를 통과하게 된다.\n",
    "4. **네번째 레이어*: `Dense(1, activation=tf.nn.sigmoid)`, 이전 레이어는 하나의 output node와 연결되고, `sigmoid` 활성함수(activation function)을 사용해 0과 1의 사이의 float을 얻게되며, 이 float는 확률 혹은 confidence level을 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a validation set\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a graph of accuracy and loss over time\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow10",
   "language": "python",
   "name": "tensorflow10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
